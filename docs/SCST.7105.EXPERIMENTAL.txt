--- /home/dave/orig/scst/iscsi-scst/kernel/nthread.c	2017-03-23 14:21:35.702586221 -0600
+++ scst/iscsi-scst/kernel/nthread.c	2017-03-18 01:06:47.426524629 -0600
@@ -1039,6 +1039,153 @@
 	return res;
 }
 
+#ifdef SCST_USERMODE		// Enable adaptive Nagle optimization
+
+/* XXX I expect the ADAPTIVE_NAGLE logic to require some modification to work well
+ * when SCST is resident in the kernel and the scst_do_job_rd() function is in use;
+ * because in that configuration one thread can service multiple sessions from that
+ * function, only going to sleep (and subsequently incurring the scheduling
+ * latency) when all sessions become idle concurrently.
+ *
+ * The algorithm here assumes the differing model seen in the usermode build, where
+ * there is one Session Thread per session, which sleeps when the session is idle.
+ * I *would* expect the logic here to show the same benefit in the kernel for the
+ * case where the server is handling a single session using a single reader thread.
+ *
+ * Probably the algorithm could be modified to also handle the shared session-
+ * thread model used in the kernel.  I would expect its benefits to be most visible
+ * in kernel-based SCST when running a single session to the target.
+ */
+#define ADAPTIVE_NAGLE	1	/* dynamically enable and disable Nagle mode */
+#define USE_QUICKACK	1	/* set quickack to help very low queue depths */
+
+/* Stats printed in conn.c */
+#define CPU_BUSY_STATS	1   /* add stats relevant to ADAPTIVE_NAGLE to conn dump */
+#define CPU_BUSY_HISTO	1   /* noisy histogram of max BUSY count reached each IDLE */
+
+//TUNE: These only affect workloads with a CPU-bound Session Thread.  Tuned for
+//	reasonable behavior at all three CPU-bound I/O sizes (512, 1024, 1536 byte)
+//	using single-session Read over 1Gb Ethernet MTU=9000 CPU 2400 MHz.  Some
+//	compromise between I/O sizes in the tuning, leaving room for more clever
+//	decision criteria.  I would not expect it to depend on network bandwidth or
+//	CPU speed, as those seem inherent in the "CPU-bound" test.  Probably these
+//	should be adjustable via sysfs/procfs.  XXX
+static unsigned long nagle_threshold = 96;  /* count needed to enter Nagle mode */
+static unsigned long nagle_margin = 40;	    /* additional count to stay there */
+
+static unsigned long quickack_threshold = 0;
+
+#endif	/* SCST_USERMODE */
+
+/* Set socket optimization parameters for test purposes */
+static unsigned char params_set = 0;
+static inline void set_params(void)
+{
+	if (params_set) return;
+	params_set = 1;
+
+#ifdef ADAPTIVE_NAGLE
+	string_t s1;
+	if ((s1=getenv("nagle_threshold"))) nagle_threshold = atol(s1);
+	PRINT_INFO("nagle_threshold=%lu", nagle_threshold);
+	if ((s1=getenv("nagle_margin"))) nagle_margin = atol(s1);
+	PRINT_INFO("nagle_margin=%lu", nagle_margin);
+#endif
+#ifdef USE_QUICKACK
+	string_t s2;
+	if ((s2=getenv("quickack_threshold"))) quickack_threshold = atol(s2);
+	PRINT_INFO("quickack_threshold=%lu", quickack_threshold);
+#endif
+}
+
+#ifdef ADAPTIVE_NAGLE
+/* Disable the Nagle algorithm */
+static inline void enable_tcp_nodelay(struct socket * sock)
+{
+	mm_segment_t const oldfs = get_fs();
+	set_fs(get_ds());
+	int opt = 1;
+	sock->ops->setsockopt(sock, SOL_TCP, TCP_NODELAY,
+			      (void __force __user *)&opt, sizeof(opt));
+	set_fs(oldfs);
+}
+
+/* Enable the Nagle algorithm */
+static inline void disable_tcp_nodelay(struct socket * sock)
+{
+	mm_segment_t const oldfs = get_fs();
+	set_fs(get_ds());
+	int opt = 0;
+	sock->ops->setsockopt(sock, SOL_TCP, TCP_NODELAY,
+			      (void __force __user *)&opt, sizeof(opt));
+	set_fs(oldfs);
+}
+
+/* The conn->cpu_busy_count[] array holds the maximum busy_counts attained by the
+ * last few IDLE-BUSY cycles.  cpu_busy_idx points at the "current" count, with
+ * counts of prior cycles recorded circularly in the array starting from there.
+ *
+ * cpu_busy_count_ptr(conn, 0) returns a pointer to the count for the current
+ * cycle; cpu_busy_count_ptr(conn, 1) for the last previous cycle, etc.
+ */
+static inline uint64_t * cpu_busy_count_ptr(struct iscsi_conn * conn, unsigned int idx)
+{
+    assert(idx < ARRAY_SIZE(conn->cpu_busy_count));
+    return &conn->cpu_busy_count[(conn->cpu_busy_idx + idx) % ARRAY_SIZE(conn->cpu_busy_count)];
+}
+
+/* Return the busy_count for the current IDLE-BUSY cycle */
+static inline uint64_t cpu_busy_count_get(struct iscsi_conn * conn)
+{
+    return *cpu_busy_count_ptr(conn, 0);
+}
+
+/* Increment and return the busy_count for the current IDLE-BUSY cycle */
+static inline uint64_t cpu_busy_count_incr(struct iscsi_conn * conn)
+{
+    return ++*cpu_busy_count_ptr(conn, 0);
+}
+
+/* Advance the circular history index for a new IDLE-BUSY cycle -- note this means
+ * decrementing it so positive offsets from it refer to increasingly remote cycles.
+ */
+static inline void cpu_busy_count_new(struct iscsi_conn * conn)
+{
+    conn->cpu_busy_idx = (conn->cpu_busy_idx ?: ARRAY_SIZE(conn->cpu_busy_count)) - 1;
+    *cpu_busy_count_ptr(conn, 0) = 0;
+}
+
+/* Return the average busy count of the last few IDLE-BUSY cycles */
+static inline uint64_t cpu_busy_count_avg(struct iscsi_conn * conn)
+{
+    uint64_t sum = 0;
+    int i;
+    for (i = 0; i < ARRAY_SIZE(conn->cpu_busy_count); i++) {
+	sum += *cpu_busy_count_ptr(conn, i);
+    }
+    return sum / ARRAY_SIZE(conn->cpu_busy_count);
+}
+
+#endif
+
+#ifdef USE_QUICKACK
+/* Do not defer the next TCP ACK back to the Initiator */
+static inline void do_quickack(struct socket * sock)
+{
+	mm_segment_t const oldfs = get_fs();
+	set_fs(get_ds());
+	int opt = 1;
+	sock->ops->setsockopt(sock, SOL_TCP, TCP_QUICKACK,
+			      (void __force __user *)&opt, sizeof(opt));
+	set_fs(oldfs);
+}
+#endif
+
+#ifdef CPU_BUSY_HISTO
+unsigned long busy_histogram[16*512];	    /* multiple of 16, please */
+size_t busy_histogram_len = sizeof(busy_histogram) / sizeof(busy_histogram[0]);
+#endif
+
 /* Called under CONN->rd_lock and BHs disabled, but will drop it inside, then
  * reacquire.  Returns -1 if conn closed (if so do not reference it further).
  *
@@ -1080,6 +1227,8 @@
 #endif
 	conn_rd_unlock(conn);	/* we own conn read by _STATE_PROCESSING now */
 
+	set_params();	    //XXX
+
 	rc = process_read_io(conn, &closed);
 		/*** Note that conn may now no longer exist ***/
 
@@ -1102,8 +1251,87 @@
 	/* Modify state from PROCESSING and return held lock to caller */
 	if ((rc == 0) || conn->rd_data_ready) {	    /*** Received a message ***/
 	    conn->rd_state = rd_state_previous;
+#ifdef ADAPTIVE_NAGLE
+	    cpu_busy_count_incr(conn);	/* tally another read since last idle */
+  #ifdef CPU_BUSY_STATS
+	    /* highest number of ops seen between two idles since last stat print */
+	    if (conn->max_cpu_busy_count < cpu_busy_count_get(conn)) {
+		conn->max_cpu_busy_count = cpu_busy_count_get(conn);
+	    }
+	    ++conn->stat_cpu_busy_count;    /* another read since last stat print */
+  #endif
+	    if (conn->tcp_nagle) {
+  #ifdef CPU_BUSY_STATS
+		/* tally another read in Nagle mode since last stat print */
+		++conn->read_nagle;
+  #endif
+	    } else {
+  #ifdef CPU_BUSY_STATS
+		/* tally another read in NODELAY mode since last stat print */
+		++conn->read_nodelay;
+  #endif
+		/* We are currently in NODELAY mode -- if we seem to be streaming,
+		 * enable Nagle's algorithm, which will remain enabled until the
+		 * next time we go idle wanting another incoming request.  Until
+		 * then the Session Thread will be CPU-bound.
+		 *
+		 * To avoid entering Nagle mode too early due to spikes in the
+		 * busy_count:  not only must we cross the threshold, the previous
+		 * few cycles must have averaged at least half the threshold.
+		 *
+		 * XXXX cycles that started in Nagle mode from the outset should count
+		 * all their cycles against the margin, which is the survival count
+		 * needed while running in Nagle mode to stay there when going IDLE.
+		 */
+		if (cpu_busy_count_get(conn) >= nagle_threshold &&
+			(nagle_threshold == 0 ||
+			    cpu_busy_count_avg(conn) >= (nagle_threshold + nagle_margin) / 2)) {
+		    disable_tcp_nodelay(conn->sock);	/* enable Nagle's algorithm */
+		    conn->tcp_nagle = 1;
+		}
+	    }
+#endif
 	} else {				    /*** Awaiting a message ***/
 	    conn->rd_state = ISCSI_CONN_RD_STATE_IDLE;
+#ifdef ADAPTIVE_NAGLE
+	    /* End of a string of work -- flush any partial output packets now */
+	    if (conn->tcp_nagle && nagle_threshold > 0) {
+		/* Disable Nagle and flush accumulating partial packet */
+		enable_tcp_nodelay(conn->sock);
+		/* This uses memory of the last few IDLE-BUSY cycles to facilitate
+		 * early return to Nagle mode of a generally-CPU-bound stream
+		 * resuming after a temporary interruption.  Often there will be a
+		 * pattern with most of QD arriving in one cycle and one or two
+		 * other OPs arriving in a cycle by themselves.  If this cycle or
+		 * any of the last few cycles survived *beyond the margin* after
+		 * Nagle's mode was enabled, leave it enabled so the next cycle
+		 * will begin with it that way from the outset.
+		 */
+		if (	    *cpu_busy_count_ptr(conn, 0) >= nagle_threshold + nagle_margin ||
+			    *cpu_busy_count_ptr(conn, 1) >= nagle_threshold + nagle_margin ||
+			    *cpu_busy_count_ptr(conn, 2) >= nagle_threshold + nagle_margin) {
+		    disable_tcp_nodelay(conn->sock);	/* reenable Nagle */
+		} else conn->tcp_nagle = 0;
+	    }
+#endif
+#ifdef USE_QUICKACK
+	    /* The quickack call only helps very small queue depths */
+	    if (cpu_busy_count_get(conn) <= quickack_threshold) {
+		do_quickack(conn->sock);
+	    }
+#endif
+#ifdef CPU_BUSY_STATS
+#ifdef CPU_BUSY_HISTO
+	    unsigned int N = busy_histogram_len;
+	    unsigned int i = cpu_busy_count_get(conn);
+	    i = i < N ? i : N - 1;
+	    ++busy_histogram[i];
+#endif
+	    ++conn->cpu_idles;	    /* idle periods since last stats print */
+#endif
+#ifdef ADAPTIVE_NAGLE
+	    cpu_busy_count_new(conn);	/* advance the busy_count history index */
+#endif
 	}
 
 	TRACE_EXIT();

--- /home/dave/orig/scst/iscsi-scst/kernel/conn.c	2017-03-23 14:19:50.440650827 -0600
+++ scst/iscsi-scst/kernel/conn.c	2017-03-18 01:06:47.426524629 -0600
@@ -91,12 +91,91 @@
 	return pos;
 }
 
+#ifdef SCST_USERMODE	    //XXX move these to a common .h file!
+#define ADAPTIVE_NAGLE	1
+#define CPU_BUSY_STATS	1
+#define CPU_BUSY_HISTO	1
+#endif
+
+#ifdef CPU_BUSY_HISTO
+extern unsigned long busy_histogram[];
+extern size_t busy_histogram_len;
+#endif
+
+static void print_busy_histogram(struct seq_file *seq, struct iscsi_conn *conn)
+{
+#ifdef CPU_BUSY_HISTO
+	unsigned long * H = busy_histogram;
+	unsigned int N = busy_histogram_len;
+	unsigned int printed = 0;
+	unsigned int j;
+
+	assert(N%16 == 0);
+	for (j = 0; j < N-15; j += 16) {
+	    unsigned long subtotal = 0;
+	    unsigned int i;
+	    for (i = 0; i < 16; i++) subtotal += H[j+i];
+	    if (subtotal > 0) {
+		++printed;
+		seq_printf(seq,
+		    "%4u: %8lu %8lu %8lu %8lu %8lu %8lu %8lu %8lu %8lu %8lu %8lu %8lu %8lu %8lu %8lu %8lu\n",
+		    j, H[j+0], H[j+1], H[j+ 2], H[j+ 3], H[j+ 4], H[j+ 5], H[j+ 6], H[j+ 7], 
+		       H[j+8], H[j+9], H[j+10], H[j+11], H[j+12], H[j+13], H[j+14], H[j+15]);
+	    }
+	}
+
+	if (printed) seq_printf(seq, "\n");
+
+	for (j = 0; j < N; j++) H[j] = 0;
+#endif
+}
+
+static int print_busy_state(char *p, size_t size, struct iscsi_conn *conn)
+{
+	int pos = 0;
+
+	if (conn->closing) {
+		goto out;
+	}
+
+#ifdef ADAPTIVE_NAGLE
+	pos += scnprintf(&p[pos], size - pos, "mode=%s busy=%lu",
+		 conn->tcp_nagle?"NAGLE":"NODELAY", conn->cpu_busy_count[0]);
+#endif
+#ifdef CPU_BUSY_STATS
+	{
+	unsigned long read_tot = conn->stat_cpu_busy_count?:1;
+	unsigned long pct_nagle = (100000 * conn->read_nagle + read_tot/2) / read_tot;
+	unsigned long avg_busy =
+	    (100 * conn->stat_cpu_busy_count + conn->cpu_idles/2) / (conn->cpu_idles?:1);
+
+	pos += scnprintf(&p[pos], size - pos,
+		 " max_busy=%lu avg_busy=%lu.%02lu"
+		 " idles=%lu Nagle=%lu.%03lu%% (%lu/%lu)",
+		 conn->max_cpu_busy_count,
+		 avg_busy/100, avg_busy%100, conn->cpu_idles, pct_nagle/1000,
+		 pct_nagle%1000, conn->read_nagle, conn->stat_cpu_busy_count);
+
+	/* Reset cpu-busy stats each time we print them */
+	conn->max_cpu_busy_count = 0;
+	conn->stat_cpu_busy_count = 0;
+	conn->cpu_idles = 0;
+	conn->read_nagle = 0;
+	conn->read_nodelay = 0;
+	}
+#endif
+
+out:
+	return pos;
+}
+
+
 /* target_mutex supposed to be locked */
 void conn_info_show(struct seq_file *seq, struct iscsi_session *session)
 {
 	struct iscsi_conn *conn;
 	struct sock *sk;
-	char buf[64];
+	char buf[256];
 
 	lockdep_assert_held(&session->target->target_mutex);
 
@@ -132,12 +211,16 @@
 			break;
 		}
 		seq_printf(seq, "\t\tcid:%u ip:%s ", conn->cid, buf);
+		if (print_busy_state(buf, sizeof(buf), conn)) {
+			seq_printf(seq, "busy:%s ", buf);
+		}
 		print_conn_state(buf, sizeof(buf), conn);
 		seq_printf(seq, "state:%s ", buf);
 		print_digest_state(buf, sizeof(buf), conn->hdigest_type);
 		seq_printf(seq, "hd:%s ", buf);
 		print_digest_state(buf, sizeof(buf), conn->ddigest_type);
 		seq_printf(seq, "dd:%s\n", buf);
+		print_busy_histogram(seq, conn);
 	}
 }
 

--- /home/dave/orig/scst/scst/src/dev_handlers/scst_vdisk.c	2017-03-23 14:19:50.448650975 -0600
+++ scst/scst/src/dev_handlers/scst_vdisk.c	2017-03-18 01:06:47.434524800 -0600
@@ -201,6 +201,9 @@
 	struct file *fd;
 	struct file *dif_fd;
 	struct block_device *bdev;
+#ifdef SCST_USERMODE_AIO
+	struct aio_handle * aio;
+#endif
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 30)
 	struct bio_set *vdisk_bioset;
 #endif
@@ -769,6 +772,12 @@
 
 static struct kmem_cache *blockio_work_cachep;
 
+#ifdef SCST_USERMODE_AIO
+/* Defined in scst_vdisk_aio.c #included below */
+static int vdisk_aio_attach_tgt(struct scst_tgt_dev *tgt_dev);
+static void vdisk_aio_detach_tgt(struct scst_tgt_dev *tgt_dev);
+#endif
+
 static struct scst_dev_type vdisk_blk_devtype = {
 	.name =			"vdisk_blockio",
 	.type =			TYPE_DISK,
@@ -781,8 +790,13 @@
 	.auto_cm_assignment_possible = 1,
 	.attach =		vdisk_attach,
 	.detach =		vdisk_detach,
+#ifndef SCST_USERMODE_AIO
 	.attach_tgt =		vdisk_attach_tgt,
 	.detach_tgt =		vdisk_detach_tgt,
+#else
+	.attach_tgt =		vdisk_aio_attach_tgt,
+	.detach_tgt =		vdisk_aio_detach_tgt,
+#endif
 	.parse =		non_fileio_parse,
 	.exec =			blockio_exec,
 	.on_alua_state_change_start = blockio_on_alua_state_change_start,
@@ -1156,6 +1170,10 @@
 
 	inode = file_inode(fd);
 
+#ifdef SCST_USERMODE_AIO
+	/* AIO works on both regular files and block devices */
+	if (!S_ISREG(inode->i_mode))
+#endif
 	if (blockio && !S_ISBLK(inode->i_mode)) {
 		PRINT_ERROR("File %s is NOT a block device", filename);
 		res = -EINVAL;
@@ -5680,6 +5698,12 @@
 	return res;
 }
 
+#ifdef SCST_USERMODE_AIO
+/* Defined in scst_vdisk_aio.c #included below */
+static int vdisk_fsync_blockio(loff_t loff,
+	loff_t len, struct scst_device *dev, gfp_t gfp_flags,
+	struct scst_cmd *cmd, bool async);
+#else
 static int vdisk_fsync_blockio(loff_t loff,
 	loff_t len, struct scst_device *dev, gfp_t gfp_flags,
 	struct scst_cmd *cmd, bool async)
@@ -5713,6 +5737,7 @@
 	TRACE_EXIT_RES(res);
 	return res;
 }
+#endif
 
 static int vdisk_fsync_fileio(loff_t loff,
 	loff_t len, struct scst_device *dev, struct scst_cmd *cmd, bool async)
@@ -6522,6 +6547,8 @@
 }
 #endif
 
+#ifndef SCST_USERMODE_AIO
+
 #if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 24)
 static int blockio_endio(struct bio *bio, unsigned int bytes_done, int error)
 {
@@ -6597,6 +6624,8 @@
 #endif
 }
 
+#endif /* SCST_USERMODE_AIO */
+
 static void vdisk_bio_set_failfast(struct bio *bio)
 {
 #if LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 27)
@@ -6759,6 +6788,12 @@
 #define blkdev_issue_flush(bdev, xxx) (-EPERM)
 #endif
 
+#ifdef SCST_USERMODE_AIO
+/* Implement blockio under SCST_USERMODE using aio */
+#include "scst_vdisk_aio.c"	    //XXX
+#else
+
+/* Implement blockio for a real kernel build using bio */
 static void blockio_exec_rw(struct vdisk_cmd_params *p, bool write, bool fua)
 {
 	struct scst_cmd *cmd = p->cmd;
@@ -7038,6 +7073,8 @@
 }
 #endif
 
+#endif /* SCST_USERMODE_AIO */
+
 static int vdisk_blockio_flush(struct block_device *bdev, gfp_t gfp_mask,
 	bool report_error, struct scst_cmd *cmd, bool async)
 {
@@ -11020,6 +11057,10 @@
 	if (res != 0)
 		goto out_free_null;
 
+#ifdef SCST_USERMODE_AIO
+	init_scst_vdisk_aio();
+#endif
+
 out:
 	return res;
 
@@ -11046,6 +11087,9 @@
 	exit_scst_vdisk(&vdisk_blk_devtype);
 	exit_scst_vdisk(&vdisk_file_devtype);
 	exit_scst_vdisk(&vcdrom_devtype);
+#ifdef SCST_USERMODE_AIO
+	exit_scst_vdisk_aio();
+#endif
 
 	kmem_cache_destroy(blockio_work_cachep);
 	kmem_cache_destroy(vdisk_cmd_param_cachep);
